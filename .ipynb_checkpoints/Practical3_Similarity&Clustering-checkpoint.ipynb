{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6rc4WkkPUea"
   },
   "source": [
    "# Clustering\n",
    "\n",
    "In this practical we are going to explore about unsupervised learning, and how we can apply it in the real world. Unsupervised learning involves building models without the use of labeled training data. \n",
    "\n",
    "Clustering is a widely used unsupervised learning technique, and involves the process of organisaing our data into subgroups whose features are **similar** to one another. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-1hCgz6PUei"
   },
   "source": [
    "## Similarity/correlation/dependence measure\n",
    "\n",
    "**The Similarity  measure** in machine learning is usually described as a distance with dimensions representing features of the objects. If this distance is small it will be the high degree of similarity where large distance will be the low degree of similarity. Similarity is subjective and is highly dependent on the domain and application. The relative values of each feature must be normalized or one feature could end up dominating the distance calculation. Similarity are measured in the range 0 to 1 [0,1].\n",
    "\n",
    "See <dataaspirant> (http://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/) for more detailed introduction to the most popular similarity measures and their implementation in python. \n",
    "\n",
    "See <scipy> (https://docs.scipy.org/doc/scipy-0.18.1/reference/spatial.distance.html) for all available distance metrics in scipy.spatial module. \n",
    "\n",
    "Use the cell below to play around with a couple distance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "76EBhHxoPUej"
   },
   "outputs": [],
   "source": [
    "import scipy.spatial.distance as ds\n",
    "\n",
    "u = [0, 1, 2];\n",
    "v = [3, 4, 5];\n",
    "\n",
    "print(\"euclindian %f\" %ds.euclidean(u, v)) # aka L2-Norm of (u-v)\n",
    "print(\"manhattan %f\" %ds.minkowski(u, v, p=1)) # aka L1 norm of (u-v)\n",
    "\n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MSEt7Z8qPUem"
   },
   "source": [
    "### Pearson Correlation\n",
    "\n",
    "Extent to which two or more variables fluctuate together. A positive correlation indicates the extent to which those variables increase or decrease in parallel; a negative correlation indicates the extent to which one variable increases as the other decreases.\n",
    "\n",
    "Peason Correlation can capture only linear correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pg9MHe9hPUem"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "from scipy.stats import stats\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "t = np.linspace(0, 10, 30)  #array of 30 points from 0 to 10\n",
    "yA = t*3 + 2 + np.random.normal(size=30) * 10\n",
    "yB = t*2 + np.random.normal(size=30) * 1.0\n",
    "plt.plot(t, yA, 'ro-', label='A')\n",
    "plt.plot(t, yB, 'b-', label='B')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"level\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpMETwObPUem"
   },
   "source": [
    "### Exercise: \n",
    "What is the pearson correlation coefficient between A and B?\n",
    "Make a scatter plot for A vs B, which is useful in checking correlation between two list of values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9epPlW6dPUen"
   },
   "outputs": [],
   "source": [
    "from pydoc import help\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "# help(pearsonr)\n",
    "corAB, p_value = pearsonr(yA, yB)\n",
    "print(\"Correlation: %f, p-value: %s\" %(corAB, p_value))\n",
    " \n",
    "# Your code here\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inRfI5Z0PUeo"
   },
   "outputs": [],
   "source": [
    "A = np.array([0, 2, 0, 1, 2]); B = 1-A*0.5\n",
    "plt.plot(np.arange(5), A, 'ro-')\n",
    "plt.plot(np.arange(5), B, 'bo-')\n",
    "plt.xlabel(\"index\")\n",
    "plt.ylabel(\"intensity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-dBVvqQPUeo"
   },
   "source": [
    "### Exercise: \n",
    "Compute the correlation between A and B?\n",
    "\n",
    "Do you know the answer without actually computing the correlation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHfn08HwPUep"
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzfXmUVxPUes"
   },
   "source": [
    "### Exercise \n",
    "Randomly generate two instances each with 10 values, and to make sure pearson correlation between these two instances is low (or non-significant)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "myEB5aekPUet"
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXCJUdz3PUet"
   },
   "source": [
    "## K-Means clustering with scikit-learn\n",
    "Have a look at the overview of available clustering methods in scikit-learn http://scikit-learn.org/stable/modules/clustering.html\n",
    "\n",
    "Next working on kmeans algorithm using an artificial dataset with known number of centers. \n",
    "Play with the code below and think about potential issues in applying K-means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Orap9nGKPUeu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "# Generate a 2D samples of certain number of blobs\n",
    "random_state = 170 # You can change random state and see changes in data examples)\n",
    "n_samples = 1500\n",
    "X, y = make_blobs(n_samples=n_samples, centers=3, n_features=2, random_state=random_state)\n",
    "\n",
    "# number of clusters!!!!!!\n",
    "y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "\n",
    "plt.title(\"Incorrect Number of Blobs\")\n",
    "\n",
    "# Anisotropicly distributed data\n",
    "transformation = [[ 0.60834549, -0.63667341], [-0.40887718, 0.85253229]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "y_pred = KMeans(n_clusters=3, random_state=random_state).fit_predict(X_aniso)\n",
    "print(y_pred)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.scatter(X_aniso[:, 0], X_aniso[:, 1], c=y_pred)\n",
    "plt.title(\"Anisotropicly Distributed Blobs\")\n",
    "\n",
    "y_pred = KMeans(n_clusters=2, random_state=random_state).fit_predict(X_aniso)\n",
    "\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBMVO5z7PUev"
   },
   "source": [
    "### Questions: \n",
    "\n",
    "- What are the potential issues in applying K-means?\n",
    "\n",
    "- How to intialise th centers? (What is method that Scikit-learn implemented for K-means for center initialisation?\n",
    "\n",
    "- How to determine the number of centers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRlElYEIPUew"
   },
   "source": [
    "### Exercise (not a priority in the class, do it if you have time): perform the above clustering task using a hierachical method.\n",
    "\n",
    "Hint: \n",
    "- from sklearn.cluster import AgglomerativeClustering\n",
    "- help(AgglomerativeClustering)\n",
    "- or go to scikit-learn online document for an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "WxfQYhIXPUew"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Roco7zGuPUex"
   },
   "source": [
    "## K-means for VQ based image compression \n",
    "\n",
    "K-means clustering can be used for vector quantisation (VQ). It attempts to partition ```n``` observations into ```k``` clusters in which each observation belongs to a cluster of the nearest mean. This results in partitioning the data space into Voronoi cells.\n",
    "\n",
    "![voroni_cell](https://upload.wikimedia.org/wikipedia/commons/e/e5/KMeans-Gaussian-data.svg)\n",
    "\n",
    "VQ is the procedure of calculating continuous with discrete values as a method of deducing a compact representation data whilst mantaining a fair representation of the dataset. \n",
    "\n",
    "A working example of this can be found below. Study and modify the code to get a better understanding of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9IYvMcjPUex"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy.misc import face\n",
    "photo = face(gray=True)\n",
    "\n",
    "print(\"Raw Image\")\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(photo, cmap=plt.cm.gray)\n",
    "plt.show()\n",
    "\n",
    "# The number of clusters we wish to deduce.\n",
    "# Change this to better understand the methodology.\n",
    "n_clusters = 2\n",
    "\n",
    "# Flatten the image\n",
    "# Convert from photo of shape (768, 1024)) to X of shape (N,1)\n",
    "X = photo.reshape((-1, 1)) \n",
    "km = KMeans(n_clusters=n_clusters)\n",
    "km.fit(X)\n",
    "# Get compressed values\n",
    "values = km.cluster_centers_.squeeze()\n",
    "# And now the labels\n",
    "labels = km.labels_\n",
    "\n",
    "# Now, select the important features\n",
    "photo_compressed = np.choose(labels, values)\n",
    "photo_compressed.shape = photo.shape\n",
    "\n",
    "print(\"Compressed Image\")\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(photo_compressed, cmap=plt.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0cPDaVDPUf6"
   },
   "source": [
    "### Question: do you know how and why clustering can be applied for image compression now? \n",
    "\n",
    "If not, more explanation can be found below.  \n",
    "\n",
    "For example, to store color intensities you can quantise floating-point values in the range ```[0.0, 1.0]``` to integer values in the range ```0-255```, representing them with 8 bits, which is considered a sufficient resolution for many applications dealing with color. In this example, the spacing of possible values is the same over the entire discrete set, so we speak of uniform quantisation; often, a nonuniform spacing is more appropriate when better resolution is needed over some parts of the range of values. Floating-point number representation is an example of nonuniform quantization - you have the as many possible FP values between 0.1 and 1 as you have between 10 and 100.\n",
    "\n",
    "Both these are examples of scalar quantisation the input **and** output values are scalars, or single numbers. By quantising each component of the vector for itself, we gain nothing over standard scalar quantisation; however, if we quantize the **entire vectors**, opting to replace them with vectors from a carefully selected sparse nonuniform set and storing just indices into that set, we can deduce a compressed representation of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuvxXhaWPUf7"
   },
   "source": [
    "## Gaussian Mixture Models\n",
    "\n",
    "A Mixture Model is a class of probability density models in which the data is assumed to be formed by a number of component distributions. Where these distributions are Guassian, then the model forms a Gaussian Mixture Model. These distributions are combined to produce a multi-modal density function, thus forming a mixture model.\n",
    "\n",
    "You can think of mixture models as generalising *k*-means clustering to incorporate information about the covariance structure of the data as well as the centers of found latent Gaussians.\n",
    "\n",
    "Familarise yourself with ```sklearn```'s documentation for the Gaussian Mixture model:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture\n",
    "\n",
    "### The Expectation–Maximisation Algorithm\n",
    "\n",
    "The more perceptive of you may have noticed the following paragraph linked in the aforementioned documentation:\n",
    "\n",
    "> The ```GaussianMixture``` object implements the expectation-maximization (EM) algorithm for fitting mixture-of-Gaussian models. It can also draw confidence ellipsoids for multivariate models, and compute the Bayesian Information Criterion to assess the number of clusters in the data. A ```GaussianMixture.```fit method is provided that learns a Gaussian Mixture Model from ```train data```. Given ``test data```, it can assign to each sample the Gaussian it mostly probably belong to using the ```GaussianMixture.predict``` method.\n",
    "\n",
    "A major difficuly in fitting Gaussian mixture models in an unsupervised fashion is that you may not know what points are generated from a given latent component. The Expectation-Maximisation algorithm is a well-founded method to get around this problem through an iterative process.\n",
    "\n",
    "1. We assume random components (in the form of randomly centered data points learned from k-means), and compute for each point a probability of being generated by each component of the model.\n",
    "2. Tweak the parameters to maximise the liklihood of the data belonging to those assignments.\n",
    "\n",
    "This simplistic technique is almost guaranteed to converge to a local optimum. See \n",
    "https://alliance.seas.upenn.edu/~cis520/dynamic/2020/wiki/index.php?n=Lectures.EM\n",
    "\n",
    "https://scikit-learn.org/stable/modules/mixture.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7toWWFMPUf8"
   },
   "source": [
    "#### Exercise: \n",
    "Perform clustering using GMM on the blob or iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "CdknE7qlPUf8"
   },
   "outputs": [],
   "source": [
    "# Exercise on GMM for clustering \n",
    "\n",
    "# Clustering using GMM on the blob datast that we just played with ealier.  \n",
    "# Your code here: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHESXtqvPUf8"
   },
   "source": [
    "\n",
    "### Unsupervised learning for the compound data in the Kaggle challenge \n",
    "\n",
    "Last week we attempted to classify compounds through the use of chemical structural information. It would be also useful to perform clustering using the compound data either on the training set or test set, and examining the possible clusters of chemical compounds and their characteristics that might be linked to BBB permeability. \n",
    "\n",
    "### Question:\n",
    "What would be the main consideration if you would have to perform clustering on this dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "knt-CwvVPUf9"
   },
   "source": [
    "### Additional readings on how to determine the number of clusters\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set\n",
    "- https://scikit-learn.org/stable/auto_examples/cluster/plot_mean_shift.html#sphx-glr-auto-examples-cluster-plot-mean-shift-py\n",
    "- http://efavdb.com/mean-shift/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAfk3R70Vfs7"
   },
   "source": [
    "### Additional coding exercise for implementing EM for GMM\n",
    "- Follow a different notebook [EM_for_gmm.ipynb](https://colab.research.google.com/drive/16Lsisv5hjxzd9FXgqHwIFyglgyNvZoxR?usp=sharing) for exercise. \n",
    "\n",
    "\n",
    "- Solutions: [EM_for_gmm_sol.ipynb](https://colab.research.google.com/drive/1W_Ais-YYwAZPNv3JKy2nCW7nSQQmAAbI?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practical3-Similarity&Clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
